# アーキテクチャ概要

以下は、本リポジトリが想定するLLMベース軽量レコメンドエンジンのアーキテクチャを示したMermaid図です。処理フローではなく、各コンポーネント間の構成を表現しています。

```mermaid
graph TD
    subgraph クライアント
        U[ユーザー]
    end
    subgraph APIサーバ
        A[レコメンドAPI]
        B[候補取得・ソート]
        C[ランダム挿入]
        D[LLMスコアリング\n(並列実行)]
        E[早期停止判定]
        F[結果生成]
    end
    subgraph 外部サービス
        L[LLMサービス]
        DB[(候補データベース)]
    end

    U -->|リクエスト| A
    A --> B
    B --> C
    C --> D
    D --> L
    D --> E
    E --> F
    B <-- DB
    F -->|レスポンス| U
```

この構成では、APIサーバ上で候補取得からLLMスコアリング、早期停止判定までを一連のパイプラインとして実行します。LLMへの問い合わせは並列化され、所定の閾値を満たす候補がk件見つかればEの段階で処理を終了します。最後にFで結果を整形し、ユーザーへ返却します。
